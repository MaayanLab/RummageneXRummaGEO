{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/rummagenexrummageo.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_pmid_from_pmc(pmc_ids, max_retries=10):\n",
    "    rar = {}\n",
    "    ids = ','.join(pmc_ids)\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids={ids}&tool=my_tool&email=my_email@example.com\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the response\n",
    "            data = xmltodict.parse(response.content)\n",
    "            \n",
    "            # Extract PubMed ID\n",
    "            if 'record' in data[\"pmcids\"]:\n",
    "                pmc_data = data[\"pmcids\"]['record']\n",
    "                if isinstance(pmc_data, list):\n",
    "                    for dicte in pmc_data:\n",
    "                        if \"@pmid\" in dicte:\n",
    "                            rar[dicte[\"@requested-id\"]] = dicte[\"@pmid\"]\n",
    "                            # lala.append(dicte[\"@pmid\"])\n",
    "                        else:\n",
    "                            rar[dicte[\"@requested-id\"]] = \"Empty\"\n",
    "\n",
    "                else:\n",
    "                    if \"@pmid\" in pmc_data:\n",
    "                        rar[pmc_data[\"@requested-id\"]] = pmc_data[\"@pmid\"]\n",
    "                    else:\n",
    "                        rar[pmc_data[\"@requested-id\"]] = \"Empty\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "            return rar\n",
    "        except (TypeError,KeyError) as e:\n",
    "            print(pmc_ids)\n",
    "            print(len(pmc_ids))\n",
    "            print(f\"TypeError: {data}\")\n",
    "            print(f\"TypeError: {pmc_data}\")\n",
    "\n",
    "            raise e\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}. Attempt {attempt + 1} of {max_retries}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                # Exponential backoff: wait a bit before retrying\n",
    "                wait_time = 2 ** attempt  # 2^0, 2^1, 2^2, ..., 2^(attempt-1)\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to fetch data.{pmc_ids}\")\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "def process_pmc_lst(tuples_list):\n",
    "    batch_size = 200  \n",
    "    if os.path.exists(\"data/pmids_dict.json\"):\n",
    "        with open(\"data/pmids_dict.json\", 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    tuples_list.difference_update(all_results.keys())\n",
    "    tuples_list = list(tuples_list)\n",
    "\n",
    "    total_batches = (len(tuples_list) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(list(tuples_list)), batch_size), total=total_batches, desc=\"Processing Batches\"):\n",
    "        pmc_ids= tuples_list[i:i + batch_size]\n",
    "        pmc_ids = set(pmc_ids) \n",
    "        pmc_ids.difference_update(all_results.keys())\n",
    "        if len(pmc_ids) > 0:\n",
    "            r = fetch_pmid_from_pmc(pmc_ids)\n",
    "            all_results.update(r)\n",
    "        with open(\"data/pmids_dict.json\", 'w') as f:\n",
    "            json.dump(all_results, f, indent=4)\n",
    "        time.sleep(1)  # Delay to avoid hitting rate limits\n",
    "    return all_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def empty_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Empties the specified directory if it isn't empty.\n",
    "\n",
    "    Parameters:\n",
    "    directory_path (str): The path to the directory to be emptied.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"{directory_path} is not a directory.\")\n",
    "        return\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed file: {file_path}\")\n",
    "            \n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to remove {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GEOparse\n",
    "\n",
    "\n",
    "def get_metadata_from_geo(gse_id):\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            metadata = GEOparse.get_GEO(geo=gse_id, silent=True, destdir=\"GEO\")\n",
    "            metadata = metadata.metadata\n",
    "            \n",
    "            if isinstance(metadata, str) and \"Error: Download failed due to \" in metadata:\n",
    "                raise IOError()\n",
    "            \n",
    "            empty_directory(\"GEO\")\n",
    "            return metadata\n",
    "        \n",
    "        except IOError as e:\n",
    "            print(f\"Attempt {i+1}/10: Error downloading GEO data: {e}\")\n",
    "            \n",
    "            if i == 9:  # On the last attempt, write to the file\n",
    "                with open(\"data/GSEs_to_delete.txt\", 'a') as file:\n",
    "                    file.write(f\"{gse_id}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            # Handle or log the unexpected error as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_by_dict(df, col1, col2, lookup_dict):\n",
    "    \"\"\"\n",
    "    Filters out rows from the DataFrame where the value in `col1` is present \n",
    "    in the dictionary values whose key is the value in `col2`. Goal is to remove redundant studies\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    def row_should_keep(row):\n",
    "        key = row[col2]\n",
    "        value = row[col1]\n",
    "        # Check if the value is in the dictionary's value for the given key\n",
    "        return not (key in lookup_dict and value in lookup_dict[key])\n",
    "    \n",
    "    tqdm.pandas()  \n",
    "    df_filtered = df[df.progress_apply(row_should_keep, axis=1)]    \n",
    "    return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"geo_gse\"] = df[\"rummageo\"].str.split(\"-\").str[0]\n",
    "df[\"geo_gse\"] = df[\"geo_gse\"].str.split(\",\").str[0]\n",
    "df[\"pmc_id\"] = df[\"rummagene\"].str.split(\"-\").str[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "def process_geos_meta(lst):\n",
    "    lst = set(lst)\n",
    "    if os.path.exists(\"data/gse_results.json\"):\n",
    "        with open(\"data/gse_results.json\", 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "    lst.difference_update(all_results.keys())\n",
    "    lst = list(lst)\n",
    "    if len(lst) > 0:\n",
    "        for ele in tqdm(lst, desc=\"Processing items\"):\n",
    "            all_results[ele] = get_metadata_from_geo(ele)\n",
    "            with open(\"data/gse_results.json\", 'w') as f:\n",
    "                json.dump(all_results, f, indent=4)\n",
    "    return\n",
    "\n",
    "\n",
    "def process_geos_pmids(lst):\n",
    "    lst = set(lst)\n",
    "\n",
    "    if not os.path.exists(\"data/gse_results.json\"):\n",
    "        print(\"Error: File 'data/gse_results.json' does not exist.\")\n",
    "        return\n",
    "\n",
    "    with open(\"data/gse_results.json\", 'r') as f:\n",
    "        all_res = json.load(f)\n",
    "\n",
    "    try:\n",
    "        pmids = {\n",
    "            ele: all_res[ele].get(\"pubmed_id\", [\"Empty\"])\n",
    "            for ele in lst\n",
    "            if isinstance(all_res.get(ele), dict)  \n",
    "        }\n",
    "\n",
    "        with open(\"data/pmids_dict_gse.json\", 'w') as f:\n",
    "            json.dump(pmids, f, indent=4)\n",
    "\n",
    "        return \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_geos_meta(list(df[\"geo_gse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"data/GSEs_to_delete.txt\", 'r') as file:\n",
    "        gse_to_delete = {line.strip() for line in file} \n",
    "        print(gse_to_delete)\n",
    "    df= df[~df[\"geo_gse\"].isin(gse_to_delete)]\n",
    "    df\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = set(list(df[\"geo_gse\"]))\n",
    "retrieved_list = list(lst)\n",
    "process_geos_pmids(retrieved_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmc_pmids = process_pmc_lst(set(df[\"pmc_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pmids_dict.json\", 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "df['pmid'] = df['pmc_id'].map(all_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pmids_dict_gse.json\", 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_by_dict(df, \"pmid\", \"geo_gse\", all_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pmids_dict_gse.json\") as f:\n",
    "    gses = json.load(f)\n",
    "gses = {ele:','.join(gses[ele]) for ele in gses}\n",
    "df[\"geo_pmid\"] = df[\"geo_gse\"].map(gses)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df.drop_duplicates(subset=[\"rummagene\",\"rummageo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/rummagenexrummageo.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
