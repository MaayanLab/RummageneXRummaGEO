{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/rummagenexrummageo_sym.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv(\"data/rummagenexrummageo.csv\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/new_repo/RummageneXRummaGEO/myvenv/lib/python3.10/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m~/Downloads/new_repo/RummageneXRummaGEO/myvenv/lib/python3.10/site-packages/pandas/core/api.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     NaT,\n\u001b[1;32m      3\u001b[0m     Period,\n\u001b[1;32m      4\u001b[0m     Timedelta,\n\u001b[1;32m      5\u001b[0m     Timestamp,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/new_repo/RummageneXRummaGEO/myvenv/lib/python3.10/site-packages/pandas/_libs/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     NaT,\n\u001b[1;32m     21\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     iNaT,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmissing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Downloads/new_repo/RummageneXRummaGEO/myvenv/lib/python3.10/site-packages/pandas/_libs/tslibs/__init__.py:39\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalize_pydatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_supported_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m ]\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes  \u001b[38;5;66;03m# pylint: disable=import-self\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m localize_pydatetime\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     42\u001b[0m     Resolution,\n\u001b[1;32m     43\u001b[0m     periods_per_day,\n\u001b[1;32m     44\u001b[0m     periods_per_second,\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32mdtypes.pyx:155\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.dtypes\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/rummagenexrummageo_sym.csv\")\n",
    "\n",
    "# df = pd.read_csv(\"data/rummagenexrummageo.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_pmid_from_pmc(pmc_ids, max_retries=10):\n",
    "    rar = {}\n",
    "    ids = ','.join(pmc_ids)\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids={ids}&tool=my_tool&email=my_email@example.com\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse the response\n",
    "            data = xmltodict.parse(response.content)\n",
    "            \n",
    "            # Extract PubMed ID\n",
    "            if 'record' in data[\"pmcids\"]:\n",
    "                pmc_data = data[\"pmcids\"]['record']\n",
    "                if isinstance(pmc_data, list):\n",
    "                    for dicte in pmc_data:\n",
    "                        if \"@pmid\" in dicte:\n",
    "                            rar[dicte[\"@requested-id\"]] = dicte[\"@pmid\"]\n",
    "                            # lala.append(dicte[\"@pmid\"])\n",
    "                        else:\n",
    "                            rar[dicte[\"@requested-id\"]] = \"Empty\"\n",
    "\n",
    "                else:\n",
    "                    if \"@pmid\" in pmc_data:\n",
    "                        rar[pmc_data[\"@requested-id\"]] = pmc_data[\"@pmid\"]\n",
    "                    else:\n",
    "                        rar[pmc_data[\"@requested-id\"]] = \"Empty\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "            return rar\n",
    "        except (TypeError,KeyError) as e:\n",
    "            print(pmc_ids)\n",
    "            print(len(pmc_ids))\n",
    "            print(f\"TypeError: {data}\")\n",
    "            print(f\"TypeError: {pmc_data}\")\n",
    "\n",
    "            raise e\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}. Attempt {attempt + 1} of {max_retries}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                # Exponential backoff: wait a bit before retrying\n",
    "                wait_time = 2 ** attempt  # 2^0, 2^1, 2^2, ..., 2^(attempt-1)\n",
    "                print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to fetch data.{pmc_ids}\")\n",
    "                return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "def process_pmc_lst(tuples_list):\n",
    "    batch_size = 200  \n",
    "    if os.path.exists(\"data/pmids_dict.json\"):\n",
    "        with open(\"data/pmids_dict.json\", 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "\n",
    "    tuples_list.difference_update(all_results.keys())\n",
    "    tuples_list = list(tuples_list)\n",
    "\n",
    "    total_batches = (len(tuples_list) + batch_size - 1) // batch_size\n",
    "    for i in tqdm(range(0, len(list(tuples_list)), batch_size), total=total_batches, desc=\"Processing Batches\"):\n",
    "        pmc_ids= tuples_list[i:i + batch_size]\n",
    "        pmc_ids = set(pmc_ids) \n",
    "        pmc_ids.difference_update(all_results.keys())\n",
    "        if len(pmc_ids) > 0:\n",
    "            r = fetch_pmid_from_pmc(pmc_ids)\n",
    "            all_results.update(r)\n",
    "        with open(\"data/pmids_dict.json\", 'w') as f:\n",
    "            json.dump(all_results, f, indent=4)\n",
    "        time.sleep(1)  # Delay to avoid hitting rate limits\n",
    "    return all_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def empty_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Empties the specified directory if it isn't empty.\n",
    "\n",
    "    Parameters:\n",
    "    directory_path (str): The path to the directory to be emptied.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"{directory_path} is not a directory.\")\n",
    "        return\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed file: {file_path}\")\n",
    "            \n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to remove {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GEOparse\n",
    "\n",
    "\n",
    "def get_metadata_from_geo(gse_id):\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            metadata = GEOparse.get_GEO(geo=gse_id, silent=True, destdir=\"GEO\")\n",
    "            metadata = metadata.metadata\n",
    "            \n",
    "            if isinstance(metadata, str) and \"Error: Download failed due to \" in metadata:\n",
    "                raise IOError()\n",
    "            \n",
    "            empty_directory(\"GEO\")\n",
    "            return metadata\n",
    "        \n",
    "        except IOError as e:\n",
    "            print(f\"Attempt {i+1}/10: Error downloading GEO data: {e}\")\n",
    "            \n",
    "            if i == 9:  # On the last attempt, write to the file\n",
    "                with open(\"data/GSEs_to_delete.txt\", 'a') as file:\n",
    "                    file.write(f\"{gse_id}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            # Handle or log the unexpected error as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_by_dict(df, col1, col2, lookup_dict):\n",
    "    \"\"\"\n",
    "    Filters out rows from the DataFrame where the value in `col1` is present \n",
    "    in the dictionary values whose key is the value in `col2`. Goal is to remove redundant studies\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    def row_should_keep(row):\n",
    "        key = row[col2]\n",
    "        value = row[col1]\n",
    "        # Check if the value is in the dictionary's value for the given key\n",
    "        return not (key in lookup_dict and value in lookup_dict[key])\n",
    "    \n",
    "    tqdm.pandas()  \n",
    "    df_filtered = df[df.progress_apply(row_should_keep, axis=1)]    \n",
    "    return df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"geo_gse\"] = df[\"rummageo\"].str.split(\"-\").str[0]\n",
    "df[\"geo_gse\"] = df[\"geo_gse\"].str.split(\",\").str[0]\n",
    "df[\"pmc_id\"] = df[\"rummagene\"].str.split(\"-\").str[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "def process_geos_meta(lst):\n",
    "    lst = set(lst)\n",
    "    if os.path.exists(\"data/gse_results.json\"):\n",
    "        with open(\"data/gse_results.json\", 'r') as f:\n",
    "            all_results = json.load(f)\n",
    "    else:\n",
    "        all_results = {}\n",
    "    lst.difference_update(all_results.keys())\n",
    "    lst = list(lst)\n",
    "    if len(lst) > 0:\n",
    "        for ele in tqdm(lst, desc=\"Processing items\"):\n",
    "            all_results[ele] = get_metadata_from_geo(ele)\n",
    "            with open(\"data/gse_results.json\", 'w') as f:\n",
    "                json.dump(all_results, f, indent=4)\n",
    "    return\n",
    "\n",
    "\n",
    "def process_geos_pmids(lst):\n",
    "    lst = set(lst)\n",
    "\n",
    "    if not os.path.exists(\"data/gse_results.json\"):\n",
    "        print(\"Error: File 'data/gse_results.json' does not exist.\")\n",
    "        return\n",
    "\n",
    "    with open(\"data/gse_results.json\", 'r') as f:\n",
    "        all_res = json.load(f)\n",
    "\n",
    "    try:\n",
    "        pmids = {\n",
    "            ele: all_res[ele].get(\"pubmed_id\", [\"Empty\"])\n",
    "            for ele in lst\n",
    "            if isinstance(all_res.get(ele), dict)  \n",
    "        }\n",
    "\n",
    "        with open(\"data/pmids_dict_gse.json\", 'w') as f:\n",
    "            json.dump(pmids, f, indent=4)\n",
    "\n",
    "        return \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_geos_meta(list(df[\"geo_gse\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"data/GSEs_to_delete.txt\", 'r') as file:\n",
    "        gse_to_delete = {line.strip() for line in file} \n",
    "        print(gse_to_delete)\n",
    "    df= df[~df[\"geo_gse\"].isin(gse_to_delete)]\n",
    "    df\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = set(list(df[\"geo_gse\"]))\n",
    "retrieved_list = list(lst)\n",
    "process_geos_pmids(retrieved_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmc_pmids = process_pmc_lst(set(df[\"pmc_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pmids_dict.json\", 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "df['pmid'] = df['pmc_id'].map(all_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pmids_dict_gse.json\", 'r') as f:\n",
    "        all_results = json.load(f)\n",
    "all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_by_dict(df, \"pmid\", \"geo_gse\", all_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pmids_dict_gse.json\") as f:\n",
    "    gses = json.load(f)\n",
    "gses = {ele:','.join(gses[ele]) for ele in gses}\n",
    "df[\"geo_pmid\"] = df[\"geo_gse\"].map(gses)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df.drop_duplicates(subset=[\"rummagene\",\"rummageo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_pubmed_abstract(pmid):\n",
    "    url = f'https://pubmed.ncbi.nlm.nih.gov/{pmid}/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the abstract section using PubMed's HTML structure\n",
    "        abstract_section = soup.find('div', class_='abstract-content')\n",
    "        if abstract_section:\n",
    "            abstract = abstract_section.get_text(separator=\" \", strip=True)\n",
    "            return abstract\n",
    "        else:\n",
    "            return \"Abstract not found\"\n",
    "    else:\n",
    "        return \"Failed to fetch the page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_dict = {}\n",
    "df_filt = df.head(1000000) #restrict to top 1 million since those will be served to user\n",
    "df_filt = df_filt.drop_duplicates(subset=\"pmc_id\")\n",
    "df_filt = df[[\"pmc_id\", \"pmid\"]]\n",
    "df_filt = df_filt.drop_duplicates(subset=\"pmc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df[[\"pmc_id\", \"pmid\"]]\n",
    "df_filt = df_filt.drop_duplicates(subset=\"pmc_id\")\n",
    "result_dicte = dict(zip( df_filt['pmc_id'], df_filt['pmid']))\n",
    "res_dict = {key:result_dicte[key] for key in result_dicte if key not in abs_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = \"data/output1.txt\"\n",
    "file_path2 = \"data/PMCs_to_delete.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(file_path1):\n",
    "    with open(file_path1, 'r') as f:\n",
    "        pmcs_found = {line.strip() for line in file}\n",
    "if os.path.exists(file_path2):\n",
    "    with open(file_path2, 'r') as f:\n",
    "        gses_to_be_deleted = {line.strip() for line in file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_pubmed_title_and_abstract(pmid):\n",
    "    url = f'https://pubmed.ncbi.nlm.nih.gov/{pmid}/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the title\n",
    "        title_section = soup.find('h1', class_='heading-title')\n",
    "        if title_section:\n",
    "            title = title_section.get_text(separator=\" \", strip=True)\n",
    "        else:\n",
    "            title = \"Title not found\"\n",
    "\n",
    "        # Find the abstract\n",
    "        abstract_section = soup.find('div', class_='abstract-content')\n",
    "        if abstract_section:\n",
    "            abstract = abstract_section.get_text(separator=\" \", strip=True)\n",
    "        else:\n",
    "            abstract = \"Abstract not found\"\n",
    "        \n",
    "        return title, abstract\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pmc_title_and_abstract(pmcid):\n",
    "    url = f'https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the title\n",
    "        title_section = soup.find('h1', class_='content-title')\n",
    "        if title_section:\n",
    "            title = title_section.get_text(separator=\" \", strip=True)\n",
    "        else:\n",
    "            title = \"Title not found\"\n",
    "\n",
    "        # Find the abstract\n",
    "        abstract_section = soup.find('div', class_='tsec sec')\n",
    "        if abstract_section:\n",
    "            abstract = abstract_section.get_text(separator=\" \", strip=True)\n",
    "        else:\n",
    "            abstract = \"Abstract not found\"\n",
    "\n",
    "        return title, abstract\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        raise ValueError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_count = len(abs_dict)\n",
    "for pmcid in res_dict:  \n",
    "    pmid = res_dict[pmcid]\n",
    "    if pmcid not in pmcs_found and pmcid not in gses_to_be_deleted:\n",
    "        if pmid != \"Empty\":\n",
    "            title, abstract = get_pubmed_title_and_abstract(pmid)\n",
    "            abs_dict[pmcid] = {}\n",
    "            abs_dict[pmcid][\"title\"] = title\n",
    "            abs_dict[pmcid][\"abstract\"] = abstract\n",
    "            pmcs_found.add(pmcid)\n",
    "            with open(file_path1, \"a\") as file:\n",
    "                file.write(f\"{pmcid}\\n\")\n",
    "        else:\n",
    "            try:\n",
    "                title, abstract = get_pmc_title_and_abstract(pmcid)\n",
    "                abs_dict[pmcid] = {}\n",
    "                abs_dict[pmcid][\"title\"] = title\n",
    "                abs_dict[pmcid] [\"abstract\"] = abstract\n",
    "                pmcs_found.add(pmcid)\n",
    "                with open(file_path1, \"a\") as file:\n",
    "                    file.write(f\"{pmcid}\\n\")\n",
    "            except:\n",
    "                print(pmcid)\n",
    "                gses_to_be_deleted.add(pmcid)\n",
    "                with open(file_path2, \"a\") as file:\n",
    "                    file.write(f\"{pmcid}\\n\")\n",
    "    if len(abs_dict) > curr_count:\n",
    "        with open(\"data/title_abs.json\", 'w') as f:\n",
    "                json.dump(abs_dict, f, indent=2)\n",
    "        curr_count = len(abs_dict)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['pmc_id'].isin(gse_to_delete)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/rummagenexrummageo_cleaned.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
