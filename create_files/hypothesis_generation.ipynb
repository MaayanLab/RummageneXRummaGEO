{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/rummagenexrummageo.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='pmc_id', keep='first')\n",
    "df = df.sort_values(by=[\"p-value\", \"odds\"], ascending=[True, False])\n",
    "df.index = np.arange(1, len(df)+1)\n",
    "df = df.head(1000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rummagene = pd.read_csv(\"data/rummagene.csv\")\n",
    "rummagene_dict = {row_inner[\"identifier\"]: row_inner[\"genes\"] for index_inner, row_inner in rummagene.iterrows() if row_inner[\"identifier\"] in df[\"rummagene\"].tolist()}\n",
    "rummageo_comb = pd.concat([ pd.read_csv(\"data/rummageo_human.csv\"), pd.read_csv(\"data/rummageo_mouse.csv\")],  ignore_index=True)\n",
    "rummageo_dict = {row_inner[\"identifier\"]: row_inner[\"genes\"] for index_inner, row_inner in rummageo_comb.iterrows() if row_inner[\"identifier\"] in df[\"rummageo\"].tolist()}\n",
    "df[\"rummagene_genes\"] =  df[\"rummagene\"].map(rummagene_dict)\n",
    "df[\"rummageo_genes\"] =  df[\"rummageo\"].map(rummageo_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/rummagenexrummageo_1k.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rummageo_comb\n",
    "del rummagene\n",
    "df = pd.read_csv(\"data/rummagenexrummageo_1k.csv\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"geo_gsei\"] = df[\"rummageo\"].str.split(\"-\").str[0]\n",
    "df_rummagene_map = {row[\"rummagene\"]: [row[\"rummagene-desc\"], row[\"pmc_id\"]] for _, row in df.iterrows()}\n",
    "df_rummageo_map = {row[\"rummageo\"]: [row[\"geo_gse\"], row[\"geo_gsei\"], row[\"species\"]] for _, row in df.iterrows()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/gse_results.json\" ,\"r\") as f:\n",
    "        gse_info = json.load(f)\n",
    "with open(\"../data/title_abs.json\" ,\"r\") as f:\n",
    "        pmc_sum = json.load(f)\n",
    "with open(\"../data/human-gse-processed-meta.json\") as f:\n",
    "        gse_human_info = json.load(f)\n",
    "with open(\"../data/mouse-gse-processed-meta.json\") as f:\n",
    "        gse_mouse_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_after_extension(termid):\n",
    "    pattern = r'^(.+?\\.\\w+)-+(.*)$'\n",
    "    \n",
    "    match = re.match(pattern, termid)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmc_again(pmcids):\n",
    "    pmc_dict = {}\n",
    "    for pm in pmcids:\n",
    "        pmc_dict[pm] = (pmc_sum[pmcids[pm][1]][\"title\"], pmc_sum[pmcids[pm][1]][\"abstract\"], extract_after_extension(pm), pmcids[pm][0])\n",
    "\n",
    "    return pmc_dict\n",
    "\n",
    "\n",
    "fin_pmc = pmc_again(df_rummagene_map)\n",
    "fin_pmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gse_again(gseids):\n",
    "    gse_dict = {}\n",
    "    for gse in gseids:\n",
    "        if gseids[gse][2] == \"human\":\n",
    "            dt = gse_human_info\n",
    "        else:\n",
    "            dt = gse_mouse_info\n",
    "        title = gse_info[gseids[gse][0]][\"title\"] \n",
    "        summary = gse_info[gseids[gse][0]][\"summary\"][0]\n",
    "        cond1= dt[gseids[gse][1]][\"titles\"][gse.split(\"-\")[1]] \n",
    "        cond2 = dt[gseids[gse][1]][\"titles\"][gse.split(\"-\")[3]]\n",
    "        dir = gse.split(\"-\")[4].split(\" \")[0]\n",
    "        gse_dict[gse] = (title, summary, cond1,cond2, dir)\n",
    "    return gse_dict\n",
    "\n",
    "fin_gse = gse_again(df_rummageo_map)\n",
    "fin_gse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_enrichr_terms(genes, desc):\n",
    "    ENRICHR_URL = 'https://maayanlab.cloud/Enrichr/'\n",
    "    endpoint = 'addList'\n",
    "    user_list_id = None\n",
    "    enrichr_libraries = ['WikiPathway_2023_Human', 'GWAS_Catalog_2023', 'GO_Biological_Process_2023', 'MGI_Mammalian_Phenotype_Level_4_2024']\n",
    "    \n",
    "    try:\n",
    "        gen = genes.split(\";\")\n",
    "        genes_string = '\\n'.join(gen).replace(\"'\", '')\n",
    "        response = requests.post(\n",
    "            ENRICHR_URL + endpoint,\n",
    "            files={'list': (None, genes_string), 'description': (None, desc)}\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        user_list_id = data['userListId']\n",
    "        short_id = data['shortId']\n",
    "        \n",
    "    except requests.exceptions.RequestException as error:\n",
    "        print(f\"Error: {error}\")\n",
    "        return\n",
    "\n",
    "    enriched_terms = {}\n",
    "    enrichr_stats = {}\n",
    "    enrichr_link = f'{ENRICHR_URL}view?userListId={user_list_id}'\n",
    "    enrichr_page_link = f\"https://maayanlab.cloud/Enrichr/enrich?dataset={short_id}\"\n",
    "    for enrichr_library in enrichr_libraries:\n",
    "        query_string = f'enrich?userListId={user_list_id}&backgroundType={enrichr_library}'\n",
    "        try:\n",
    "            response = requests.get(ENRICHR_URL + query_string, headers={'Accept': 'application/json'})\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            enriched_terms[enrichr_library] = []\n",
    "            for term in data[enrichr_library][:3]:  # Limit to top 3 results\n",
    "                term_name = term[1]\n",
    "                enriched_terms[enrichr_library].append(term_name)\n",
    "                enrichr_stats[term_name] = term\n",
    "                enrichr_stats[term_name].append(enrichr_library)\n",
    "\n",
    "            time.sleep(0.5)  # Delay between requests\n",
    "\n",
    "        except requests.exceptions.RequestException as error:\n",
    "            print(f\"Error: {error}\")\n",
    "            return\n",
    "\n",
    "    return enriched_terms, enrichr_stats, enrichr_link, enrichr_page_link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fetch_hypothesis(pm_title, user_desc: str, desct, desc, gse_title, gse_summary: str, cond1, cond2, species,  term2: str, enriched_terms: dict,  enriched_termsi: dict, enriched_termseo: dict,ova) -> str:\n",
    "    system_prompt = (\n",
    "        \"You are an AI hypothesis generator for RummagenexRummaGEO (gene sets from crossing Rummagene sets with RummaGEO sets). \"\n",
    "        \"You should act as a biologist in hypothesizing why a high overlap may exist between a Rummagene set (which are sets from PubMed papers) and a RummaGEO set (automatically generated signatures from Gene Expression Omnibus).\"\n",
    "    )\n",
    "\n",
    "    enriched_terms_string = \"\"\n",
    "    for library, terms in enriched_terms.items():\n",
    "        enriched_terms_string += f\"{library}: {', '.join(terms)}\\n\"\n",
    "\n",
    "    enriched_termseo_string = \"\"\n",
    "    for library, terms in enriched_termseo.items():\n",
    "        enriched_terms_string += f\"{library}: {', '.join(terms)}\\n\"\n",
    "    \n",
    "    enriched_termsi_string = \"\"\n",
    "    for library, terms in enriched_termsi.items():\n",
    "        enriched_terms_string += f\"{library}: {', '.join(terms)}\\n\"\n",
    "\n",
    "    # Generate the hypothesis\n",
    "    prompt = (\n",
    "    f\"Here are two gene sets that highly overlap. The first is from a a gene set automatically computed between two conditions in a study from the Gene Expression Omnibus (GEO).\"\n",
    "    f\"The second is a Rummagene set, which comes from a supplementary table in a published biomedical paper.\"\n",
    "    f\"Based on the term name (formatted as condition 1 vs. condition 2) and the abstract of the GEO gene set, \"\n",
    "    f\"as well as the abstract of the Rummagene gene set, hypothesize why these two gene sets have significant overlap. \"\n",
    "    f\"In your hypothesis, summarize the RummaGEO gene set and describe the Rummagene gene set. \"\n",
    "    f\"You will also be provided with enrichment results from the Enrichr database. These results show \"\n",
    "    f\"significantly overlapping functional terms from the overlapping genes of the two sets, as well as the terms from the source genes. \"\n",
    "    f\"For each enrichment term you mention, include it exactly as given (e.g., Complement And Coagulation Cascades WP558 should appear \"\n",
    "    f\"as Complement And Coagulation Cascades WP558, not as Complement And Coagulation Cascades). Do not use quotes around enrichment term names. \"\n",
    "    f\"\\n\\nGene set term 1 from RummaGEO:\\n\"\n",
    "    f'\"up\" or \"dn\" in this {term2} name indicates whether the genes were upregulated or downregulated in {cond1} vs. {cond2} conditions '\n",
    "    f\"in the signature for species {species}. Make sure to include this detail in your hypothesis.\\n\"\n",
    "    f\"Title of study for gene set term 1: {gse_title}\\n\"\n",
    "    f\"Summary of study for gene set term 1: {gse_summary}\\n\"\n",
    "    f\"Please mention these details: The enriched terms from the RummaGEO genes are: {enriched_termseo_string}\\n\\n\"\n",
    "    f\"Gene set term 2 from Rummagene:\\n\"\n",
    "    f\"Title of paper for gene set term 2: {pm_title}\\n\"\n",
    "    f\"Abstract of paper for gene set term 2: {user_desc}\\n\"\n",
    "    f\"Please mention these details: The enriched terms from the Rummagene genes are: {enriched_termsi_string}\\n\"\n",
    "    f\"{desct} is the name of the table from the paper where this gene set originates, and {desc} is its description. Include this detail if relevant.\\n\\n\"\n",
    "    f\"There are {ova} overlapping genes.\\n\"\n",
    "    f\"Enriched Terms from overlapping genes of the two sets:\\n\"\n",
    "    f\"{enriched_terms_string}\\n\\n\"\n",
    "    f\"Please compare the enriched terms from the overlapping genes ({enriched_terms_string}) to the enriched terms from the RummaGEO set \"\n",
    "    f\"({enriched_termseo_string}) and the Rummagene set ({enriched_termsi_string}). \"\n",
    "    f\"Use this comparison to support your hypothesis about why these gene sets overlap significantly, considering shared pathways or conditions.\"\n",
    ")\n",
    "\n",
    "    # Send the request to the OpenAI API\n",
    "    response = requests.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\",\n",
    "        },\n",
    "        json={\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"max_tokens\": 1000,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Parse the response from the API\n",
    "    response_json = response.json()\n",
    "    hypothesis = response_json['choices'][0]['message']['content']\n",
    "    \n",
    "    hyp = response_json['choices'][0]['message']['content']\n",
    "    title_prompt = (\n",
    "        f\"Based on the following hypothesis, generate a concise and descriptive title:\\n\\n{hyp}\"\n",
    "    )\n",
    "\n",
    "    title_response = requests.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\",\n",
    "        },\n",
    "        json={\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": title_prompt}],\n",
    "            \"max_tokens\": 50,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    )\n",
    "\n",
    "    title_json = title_response.json()\n",
    "    title = title_json['choices'][0]['message']['content'].strip()\n",
    "\n",
    "    return hypothesis, title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfe = df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/data_hyp.json'):\n",
    "        with open('../data/data_hyp.json' ,\"r\") as f:\n",
    "                hypotheses  = json.load(f)\n",
    "else:\n",
    "        hypotheses = {}\n",
    "\n",
    "if not os.path.exists('../data/data_enrichr2.json'):\n",
    "        with open('data/data_enrichr2.json',\"r\") as f:\n",
    "                enrichr_stuff= json.load(f)\n",
    "else:\n",
    "        enrichr_stuff = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import io\n",
    "import textwrap\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_bar(enriched_stats, enriched_statseo, enriched_statsi, show=False):\n",
    "    combined_data = defaultdict(lambda: {\"p_values\": [0, 0, 0], \"sources\": []})\n",
    "    \n",
    "    # Combine terms from all dictionaries\n",
    "    for idx, (source, data, color) in enumerate([\n",
    "        (\"Enriched Stats\", enriched_stats, 'blue'),\n",
    "        (\"Enriched Statsi\", enriched_statsi, 'black'),\n",
    "        (\"Enriched Statseo\", enriched_statseo, 'red')\n",
    "    ]):\n",
    "        for term, values in data.items():\n",
    "            combined_data[term][\"p_values\"][idx] = -np.log10(float(values[2]))  # Store p-value for each source\n",
    "            combined_data[term][\"sources\"].append(data[term][9])\n",
    "    \n",
    "    # Consolidate terms\n",
    "    all_terms = []\n",
    "    all_p_values = []\n",
    "    for term, info in combined_data.items():\n",
    "        all_terms.append(f\"{term} ({', '.join(set(info['sources']))})\")\n",
    "        all_p_values.append(info[\"p_values\"])\n",
    "    \n",
    "    # Sort by the total p-value (sum of stacked p-values) in descending order\n",
    "    total_p_values = [sum(pvals) for pvals in all_p_values]\n",
    "    sorted_indices = np.argsort(total_p_values)[::-1]\n",
    "    sorted_terms = [all_terms[i] for i in sorted_indices]\n",
    "    sorted_p_values = [all_p_values[i] for i in sorted_indices]\n",
    "\n",
    "    # Wrap long labels\n",
    "    wrapped_labels = ['\\n'.join(textwrap.wrap(label, width=70)) for label in sorted_terms]\n",
    "\n",
    "   \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bar_width = 0.5  \n",
    "    y_pos = range(len(sorted_terms))\n",
    "    \n",
    "    # Stack the p-values for each source\n",
    "    plt.barh(y_pos, [p[0] for p in sorted_p_values], height=bar_width, color='blue', label='Overlap')\n",
    "    plt.barh(y_pos, [p[1] for p in sorted_p_values], height=bar_width, left=[p[0] for p in sorted_p_values], color='orange', label='Rummagene')\n",
    "    plt.barh(y_pos, [p[2] for p in sorted_p_values], height=bar_width, left=[p[0] + p[1] for p in sorted_p_values], color='red', label='RummaGEO')\n",
    "\n",
    "    # Adjust font sizes\n",
    "    plt.yticks(y_pos, wrapped_labels, fontsize=11)  \n",
    "    plt.xlabel('-log10(P-value)', fontsize=11)      \n",
    "    plt.title('Enriched Terms (Stacked by Source)', fontsize=12, fontweight=\"bold\")  \n",
    "    plt.legend(\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1.05, 0.5),  \n",
    "        fontsize=10,          \n",
    "        markerscale=0.5,        \n",
    "        borderpad=0.5,          \n",
    "        frameon=True         \n",
    "    )\n",
    "\n",
    "\n",
    "    plt.gca().invert_yaxis()  \n",
    "    plt.tight_layout()\n",
    "\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer,bbox_inches=\"tight\", dpi=200)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_enrichr_tuple(tup):\n",
    "    ENRICHR_URL = 'https://maayanlab.cloud/Enrichr/'\n",
    "    ele = tup.split(\"?\")[1]\n",
    "    user_list_id = ele.split(\"=\")[1]\n",
    "    enrichr_libraries = ['WikiPathway_2023_Human', 'GWAS_Catalog_2023', 'GO_Biological_Process_2023', 'MGI_Mammalian_Phenotype_Level_4_2024']\n",
    "    enrichr_stats = {}\n",
    "    for enrichr_library in enrichr_libraries:\n",
    "        query_string = f'enrich?userListId={user_list_id}&backgroundType={enrichr_library}'\n",
    "        try:\n",
    "            response = requests.get(ENRICHR_URL + query_string, headers={'Accept': 'application/json'})\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            for term in data[enrichr_library][:3]:  # Limit to top 3 results\n",
    "                term_name = term[1]\n",
    "                enrichr_stats[term_name] = term\n",
    "                enrichr_stats[term_name].append(enrichr_library)\n",
    "\n",
    "            time.sleep(10)  # Delay between requests\n",
    "\n",
    "        except requests.exceptions.RequestException as error:\n",
    "            print(f\"Error: {error}\")\n",
    "            return\n",
    "\n",
    "    return enrichr_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "from PIL import Image\n",
    "from fpdf.enums import XPos, YPos\n",
    "\n",
    "\n",
    "def write_markdown_hypotheses_to_pdf(hypotheses, enricher, output_filename):\n",
    "    \"\"\"\n",
    "    Write a list of Markdown-formatted hypotheses to a PDF, ensuring each fits on one page.\n",
    "    \"\"\"\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.set_left_margin(15)\n",
    "    pdf.set_right_margin(15)\n",
    "    #need to download fonts\n",
    "    pdf.add_font('Arial', '', 'Arial-Unicode-Regular.TTF')\n",
    "    pdf.add_font('Arial', 'B', 'Arial-Unicode-Bold.TTF')\n",
    "    pdf.add_font('Arial', 'I', 'Arial-Unicode-Italic.TTF')\n",
    "    pdf.set_font('Arial', size=12)\n",
    "    for idx, (name, input) in enumerate(hypotheses.items()):\n",
    "            title = input[\"title\"]\n",
    "            hypothesis = input[\"hypothesis\"]\n",
    "            links = enricher[name][\"enrich view\"]\n",
    "            # Generate bar image \n",
    "            enrich = links[0][1]\n",
    "            pval = input[\"pval\"]\n",
    "            gene_size = input[\"Sizes\"][2]\n",
    "            geo_size = input[\"Sizes\"][1]\n",
    "            ova_size = input[\"Sizes\"][0]\n",
    "            pdf.add_page()\n",
    "            pdf.set_font_size(size=12)\n",
    "            pdf.multi_cell(0, 5, f\"**Hypothesis {idx + 1}: {title}**\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='C', markdown=True)\n",
    "            pdf.ln(1)\n",
    "            plain_text = hypothesis\n",
    "            parts = plain_text.split(\"\\n\\n\")\n",
    "            para_count = 0\n",
    "            gene, geo = name.split(\";\")\n",
    "            pmcid = gene.split(\"-\")[0]\n",
    "            geoid = geo.split(\"-\")[0]\n",
    "            geoid = geoid.split(\",\")[0]\n",
    "            value = str(pval)\n",
    "            if pval == 0:\n",
    "                value = \"<1e-324\"\n",
    "            pdf.set_font_size(size=11)\n",
    "            prefix_text = \"**Rummagene set:** \"\n",
    "            hyperlink_text = f\"{gene}\"\n",
    "            pdf.cell(0, 5, prefix_text, markdown=True)\n",
    "            pdf.set_text_color(0, 0, 255)  # Blue\n",
    "            pdf.set_font(style=\"U\")  # Underline\n",
    "            pdf.set_x(50)\n",
    "            pdf.multi_cell(0, 5, hyperlink_text, new_x=XPos.LMARGIN, new_y=YPos.NEXT, link=f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}\")\n",
    "            pdf.set_text_color(0, 0, 0)  \n",
    "            prefix_text = \"**RummaGEO set:**\"\n",
    "            hyperlink_text = f\"{geo}\"\n",
    "            pdf.set_font('Arial')\n",
    "            pdf.cell(0, 5, prefix_text, markdown=True)\n",
    "            pdf.set_x(50)\n",
    "            pdf.set_text_color(0, 0, 255)  # Blue\n",
    "            pdf.set_font(style=\"U\")  # Underline\n",
    "            pdf.cell(0, 5, hyperlink_text, new_x=XPos.LMARGIN, new_y=YPos.NEXT, link=f\"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc={geoid}\")\n",
    "            pdf.set_text_color(0, 0, 0) \n",
    "            #sizes\n",
    "            pdf.set_font('Arial')\n",
    "            pdf.multi_cell(0, 5, f\"**Rummagene set size**: {int(gene_size)}; **RummaGEO set size**: {int(geo_size)}\" ,new_x=XPos.LMARGIN, new_y=YPos.NEXT, markdown=True)\n",
    "            pdf.cell(0, 5, f\"**Overlap set size**: {int(ova_size)};  **p-value**: {value};  \", markdown=True)\n",
    "            #enrichr\n",
    "            pdf.set_x(92)\n",
    "            hyperlink_text = \"Enrichr Link\"\n",
    "            pdf.set_text_color(0, 0, 255)  # Blue\n",
    "            pdf.set_font(style=\"U\")  # Underline\n",
    "            pdf.cell(0, 5, hyperlink_text, new_x=XPos.LMARGIN, new_y=YPos.NEXT, link=f\"{enrich}\")\n",
    "            pdf.set_text_color(0, 0, 0) \n",
    "            pdf.ln(1)\n",
    "            pdf.set_text_color(0, 0, 0)\n",
    "            pdf.set_font('Arial', size=11)\n",
    "            for ele in parts:\n",
    "                ele.replace(\"â€”\", \"-\")\n",
    "                pdf.multi_cell(0, 5, f\"{ele}\", new_x=XPos.LMARGIN, new_y=YPos.NEXT, align=\"J\", markdown=True)\n",
    "                pdf.ln(2)\n",
    "                para_count += 1\n",
    "                if para_count == 3:\n",
    "                    bar_data = [get_enrichr_tuple(link[0]) for link in links]\n",
    "                    base64image = generate_bar(\n",
    "                        enriched_stats=bar_data[0], \n",
    "                        enriched_statseo=bar_data[1], \n",
    "                        enriched_statsi=bar_data[2], \n",
    "                        show=False\n",
    "                    )\n",
    "                    pdf.image(base64image, w=pdf.epw)  \n",
    "                    base64image.close()\n",
    "                    pdf.ln(1)\n",
    "                    pdf.set_font('Arial',size=9)\n",
    "                    legend = \"**Figure 1.** Stacked Bar Plot depicting the distribution of significantly enriched terms among the Rummagene set(yellow), the RummaGEO set(red) and the overlapping set(blue). The top 3 enriched terms from these Enrichr Libraries were used for each set: WikiPathway_2023_Human, GWAS_Catalog_2023, GO_Biological_Process_2023 and MGI_Mammalian_Phenotype_Level_4_2024\"\n",
    "                    pdf.multi_cell(0, 5, legend, new_x=XPos.LMARGIN, new_y=YPos.NEXT, align=\"L\", markdown=True)\n",
    "                    pdf.set_font('Arial', size=11)\n",
    "                    pdf.ln(1)\n",
    "\n",
    "\n",
    "\n",
    "    pdf.output(output_filename)\n",
    "write_markdown_hypotheses_to_pdf(hypotheses,enrichr_stuff, \"data/Hypotheses.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
